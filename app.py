# app.py

from flask import Flask, request, jsonify
from dotenv import load_dotenv
import os
import openai
from flask_cors import CORS
from pathlib import Path
import chromadb
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

# ğŸ” í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ğŸ§  Flask ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = Flask(__name__)
CORS(app)

# ğŸ“š í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë²¡í„° DB êµ¬ì¶• í•¨ìˆ˜
def build_vector_store(txt_path, collection_name, vector_store_path):
    print(f"[ë²¡í„° ìƒì„±] TXT: {txt_path} â†’ {vector_store_path}/{collection_name}")
    try:
        with open(txt_path, "r", encoding="utf-8") as f:
            text = f.read()
    except UnicodeDecodeError:
        with open(txt_path, "r", encoding="euc-kr") as f:
            text = f.read()

    # ì ‘ê·¼ì„± í…ìŠ¤íŠ¸ ì²˜ë¦¬
    text = text.replace("\r", "").replace("\n", " ").strip()
    chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]

    embedding_function = OpenAIEmbeddingFunction(api_key=openai.api_key)
    client = chromadb.PersistentClient(path=vector_db_path)
    collection = client.get_or_create_collection(name=collection_name, embedding_function=embedding_function)

    ids = [f"chunk_{i}" for i in range(len(chunks))]
    metadatas = [{"source": str(txt_path)}] * len(chunks)
    collection.add(documents=chunks, metadatas=metadatas, ids=ids)
    print("âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ")


# ğŸ§  ì„œë²„ ì‹œì‘ì‹œ ë²¡í„°DB êµ¬ì¶• í•¨ìˆ˜
def init_vector_db():
    base_dir = Path("manuals")
    vector_db_path = "vector_db"
    os.makedirs(vector_db_path, exist_ok=True)

    stay_files = list(base_dir.glob("*ì²´ë¥˜*.txt"))
    visa_files = list(base_dir.glob("*ì‚¬ì¦*.txt"))

    if stay_files:
        build_vector_store(stay_files[0], "stay_manual", vector_db_path)

    if visa_files:
        build_vector_store(visa_files[0], "visa_manual", vector_db_path)

# ğŸ” POST ìš”ì²­ ì²˜ë¦¬
@app.route("/search", methods=["POST"])
def search():
    question = request.json.get("question", "")

    embedding_function = OpenAIEmbeddingFunction(api_key=openai.api_key)
    client = chromadb.PersistentClient(path="vector_db")

    stay_collection = client.get_collection("stay_manual", embedding_function=embedding_function)
    stay_result = stay_collection.query(query_texts=[question], n_results=3)

    visa_collection = client.get_collection("visa_manual", embedding_function=embedding_function)
    visa_result = visa_collection.query(query_texts=[question], n_results=3)

    context_texts = []
    for result in stay_result["documents"] + visa_result["documents"]:
        context_texts.extend(result)

    context = "\n".join(context_texts)

    prompt = f"""
    ë‹¤ìŒ ë©”ë‰´ì–¼ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

    ë©”ë‰´ì–¼ ë‚´ìš©:
    {context}

    ì§ˆë¬¸:
    {question}
    """

    try:
        res = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì²´ë¥˜/ì‚¬ì¦ ì „ë¬¸ GPT ë¹„ì„œì…ë‹ˆë‹¤. ë©”ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2
        )
        answer = res.choices[0].message.content
        return jsonify({"answer": answer})
    except Exception as e:
        return jsonify({"answer": f"GPT ì˜¤ë¥˜: {str(e)}"}), 500

# âœ… ìƒíƒœ í™•ì¸ìš©
@app.route("/", methods=["GET"])
def index():
    return "âœ… ì„œë²„ ì •ìƒ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.", 200

# ğŸ”¥ ì§ì ‘ ì‹¤í–‰í•˜ê±°ë‚˜ ì„œë²„ import ì‹œ ì²˜ë¦¬
if __name__ == "__main__":
    init_vector_db()
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port, debug=True)
else:
    init_vector_db()
    application = app
