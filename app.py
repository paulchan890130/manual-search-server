# app.py

from flask import Flask, request, jsonify
from dotenv import load_dotenv
import os
import openai
from flask_cors import CORS
from pathlib import Path
from PyPDF2 import PdfReader
import chromadb
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

# ğŸ” í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ğŸ§  Flask ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = Flask(__name__)
CORS(app)

# ğŸ“š ë²¡í„° DB êµ¬ì¶• í•¨ìˆ˜
def build_vector_store(pdf_path, collection_name, vector_store_path):
    print(f"[ë²¡í„° ìƒì„±] PDF: {pdf_path} â†’ {vector_store_path}/{collection_name}")
    reader = PdfReader(pdf_path)
    text = "\n".join(page.extract_text() or "" for page in reader.pages)
    chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]

    embedding_function = OpenAIEmbeddingFunction(api_key=openai.api_key)
    client = chromadb.PersistentClient(path=vector_store_path)
    collection = client.get_or_create_collection(name=collection_name, embedding_function=embedding_function)

    ids = [f"chunk_{i}" for i in range(len(chunks))]
    metadatas = [{"source": str(pdf_path)}] * len(chunks)
    collection.add(documents=chunks, metadatas=metadatas, ids=ids)
    print("âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ")

# ğŸ§  ì„œë²„ ì‹œì‘ì‹œ ë²¡í„°DB êµ¬ì¶•
@app.before_serving
def init_vector_db():
    base_dir = Path("C:/Users/ìœ¤ì°¬/ë‚´ ë“œë¼ì´ë¸Œ/í•œìš°ë¦¬ í˜„í–‰ì—…ë¬´/í”„ë¡œê·¸ë¨/manual-search-server")
    vector_db_path = "vector_db"
    os.makedirs(vector_db_path, exist_ok=True)

    stay_files = list(base_dir.glob("*ì²´ë¥˜*.pdf"))
    visa_files = list(base_dir.glob("*ì‚¬ì¦*.pdf"))

    if stay_files and not Path(vector_db_path, "stay_manual").exists():
        build_vector_store(stay_files[0], "stay_manual", vector_db_path)

    if visa_files and not Path(vector_db_path, "visa_manual").exists():
        build_vector_store(visa_files[0], "visa_manual", vector_db_path)

# ğŸ” POST ìš”ì²­ ì²˜ë¦¬
@app.route("/search", methods=["POST"])
def search():
    question = request.json.get("question", "")

    embedding_function = OpenAIEmbeddingFunction(api_key=openai.api_key)
    client = chromadb.PersistentClient(path="vector_db")

    # ì²´ë¥˜ë¯¼ì› manual ê²€ìƒ‰
    stay_collection = client.get_collection("stay_manual", embedding_function=embedding_function)
    stay_result = stay_collection.query(query_texts=[question], n_results=3)

    # ì‚¬ì¦ë¯¼ì› manual ê²€ìƒ‰
    visa_collection = client.get_collection("visa_manual", embedding_function=embedding_function)
    visa_result = visa_collection.query(query_texts=[question], n_results=3)

    context_texts = []
    for result in stay_result["documents"] + visa_result["documents"]:
        context_texts.extend(result)

    context = "\n".join(context_texts)

    prompt = f"""
    ë‹¤ìŒ ë©”ë‰´ì–¼ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

    ë©”ë‰´ì–¼ ë‚´ìš©:
    {context}

    ì§ˆë¬¸:
    {question}
    """

    try:
        res = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì²´ë¥˜/ì‚¬ì¦ ì „ë¬¸ GPT ë¹„ì„œì…ë‹ˆë‹¤. ë©”ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2
        )
        answer = res.choices[0].message.content
        return jsonify({"answer": answer})
    except Exception as e:
        return jsonify({"answer": f"GPT ì˜¤ë¥˜: {str(e)}"}), 500

# âœ… ìƒíƒœ í™•ì¸ìš©
@app.route("/", methods=["GET"])
def index():
    return "âœ… ì„œë²„ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.", 200

# ğŸ”¥ ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ì„œë²„ ì‹œì‘
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=10000, debug=True)
else:
    application = app
